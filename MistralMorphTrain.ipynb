{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl ","metadata":{"id":"fNZZilUV_L4r","outputId":"31a2fa06-284f-411a-f439-0e8f949bfaf8","execution":{"iopub.status.busy":"2024-05-30T14:37:19.530173Z","iopub.execute_input":"2024-05-30T14:37:19.530446Z","iopub.status.idle":"2024-05-30T14:42:49.628500Z","shell.execute_reply.started":"2024-05-30T14:37:19.530420Z","shell.execute_reply":"2024-05-30T14:42:49.627382Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"id":"VLzgZ14X_rMs","execution":{"iopub.status.busy":"2024-05-30T14:42:49.630367Z","iopub.execute_input":"2024-05-30T14:42:49.630670Z","iopub.status.idle":"2024-05-30T14:43:06.408221Z","shell.execute_reply.started":"2024-05-30T14:42:49.630643Z","shell.execute_reply":"2024-05-30T14:43:06.406870Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"],"ename":"ModuleNotFoundError","evalue":"No module named 'peft'","output_type":"error"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"id":"f9oI6bPHzpDW","outputId":"07b4bb79-e35e-43a9-c0a9-fabbb4ac2721","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Monitering the LLM\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning mistral 7B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"id":"na9CAoHC5gM9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = \"mistralai/Mistral-7B-v0.3\"\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\nnew_model = \"mistral_7b_guanaco\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset[\"text\"][100]","metadata":{"id":"XzF2UjPvTBag","outputId":"3733e45f-605e-4564-88c7-368c9c5bf9cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load base model(Mistral 7B)\nbnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_4bit=True,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token\n","metadata":{"id":"StJKGiDDHzdk","outputId":"871214ba-6c30-4ecf-ac68-550f296b7ef6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding the adapters in the layers\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"id":"DZoClwHcPTqQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)\n","metadata":{"id":"peOnLAAhS0y1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True\nmodel.eval()","metadata":{"id":"nKgZBEGVS5a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    trainer.model.push_to_hub(new_model, use_temp_dir=False)\nexcept:\n    print(\"An exception occurred\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"How do I find true love?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"id":"L7vHP41ITQPb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"What is Datacamp Career track?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{},"execution_count":null,"outputs":[]}]}